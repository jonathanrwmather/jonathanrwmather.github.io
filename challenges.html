<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta charset="utf-8">
  <link rel="stylesheet" href="bootstrap.css">
  <title>Challenges</title>
</head>

<body>
  <nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="index.html">Artificial Intelligence</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNavDropdown">
      <ul class="navbar-nav">
        <li class="nav-item active">
          <a class="nav-link" href="index.html">Home<span class="sr-only">(current)</span></a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="challenges.html">Challenges</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="approaches.html">Approaches</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="tools.html">Tools</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="application.html">Application</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="philosophyandethics.html">Philosophy & ethics</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="economics.html">Economics</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="fiction.html">Fiction</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="references.html">References</a>
        </li>
        </li>
      </ul>
    </div>
  </nav>

  <main>
    <h1>Challenges Page</h1>
    <br>
    <section>
      <h2>Reasoning, problem solving</h2>

      <div class="card float-right" style="width: 12rem; height: 12rem">
        <img src="basicsImage.png" class="image-fluid" alt="basicsImage">
        <div class="card-body">
          <p class="card-text">The blue line could be an example of overfitting a linear function due to random noise.</p>
        </div>
      </div>

      <p>Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or
        incomplete information, employing concepts from probability and economics.</p>
      <p>These algorithms proved to be insufficient for solving large reasoning problems, because they experienced a "combinatorial explosion": they became exponentially slower as the problems grew larger. In fact, even humans rarely use the
        step-by-step deduction that early AI research was able to model. They solve most of their problems using fast, intuitive judgements.[84]</p>
      <br>
    </section>

    <section>
      <h2>Knowledge representation</h2>
      <p>Knowledge representation[85] and knowledge engineering[86] are central to classical AI research. Some "expert systems" attempt to gather together explicit knowledge possessed by experts in some narrow domain. In addition, some projects
        attempt
        to gather the "commonsense knowledge" known to the average person into a database containing extensive knowledge about the world. Among the things a comprehensive commonsense knowledge base would contain are: objects, properties, categories
        and
        relations between objects;[87] situations, events, states and time;[88] causes and effects;[89] knowledge about knowledge (what we know about what other people know);[90] and many other, less well researched domains. A representation of "what
        exists" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and
        typically implemented as classes, properties, and individuals in the Web Ontology Language.[91] The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge[92] by acting as mediators
        between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations can be used in content-based indexing and retrieval,[93] scene
        interpretation,[94] clinical decision support,[95] knowledge discovery (mining "interesting" and actionable inferences from large databases),[96] and other areas.[97]</p>
      <p>Among the most difficult problems in knowledge representation are:

        Default reasoning and the qualification problem
        Many of the things people know take the form of "working assumptions". For example, if a bird comes up in conversation, people typically picture an animal that is fist-sized, sings, and flies. None of these things are true about all birds.
        John
        McCarthy identified this problem in 1969 as the qualification problem: for any commonsense rule that AI researchers care to represent, there tend to be a huge number of exceptions. Almost nothing is simply true or false in the way that
        abstract
        logic requires. AI research has explored a number of solutions to this problem.[99]
        The breadth of commonsense knowledge
        The number of atomic facts that the average person knows is very large. Research projects that attempt to build a complete knowledge base of commonsense knowledge (e.g., Cyc) require enormous amounts of laborious ontological engineering—they
        must be built, by hand, one complicated concept at a time.[100]
        The subsymbolic form of some commonsense knowledge
        Much of what people know is not represented as "facts" or "statements" that they could express verbally. For example, a chess master will avoid a particular chess position because it "feels too exposed" or an art critic can take one look at a
        statue and realize that it is a fake. These are non-conscious and sub-symbolic intuitions or tendencies in the human brain. Knowledge like this informs, supports and provides a context for symbolic, conscious knowledge. As with the related
        problem of sub-symbolic reasoning, it is hoped that situated AI, computational intelligence, or statistical AI will provide ways to represent this kind of knowledge.</p>
      <br>
    </section>

    <section>
      <h2>Planning</h2>
      <p>Intelligent agents must be able to set goals and achieve them.[104] They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make
        choices that maximize the utility (or "value") of available choices.[105]

        In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.[106] However, if the agent is not the only actor, then it requires that
        the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions, but also evaluate its predictions and adapt based on its assessment.[107]

        Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.[108]</p>
      <br>
    </section>

    <section>
      <h2>Learning</h2>
      <p>Machine learning (ML), a fundamental concept of AI research since the field's inception,[109] is the study of computer algorithms that improve automatically through experience.[110][111]
        Unsupervised learning is the ability to find patterns in a stream of input, without requiring a human to label the inputs first. Supervised learning includes both classification and numerical regression, which requires a human to label the
        input data first. Classification is used to determine what category something belongs in, and occurs after a program sees a number of examples of things from several categories. Regression is the attempt to produce a function that describes
        the
        relationship between inputs and outputs and predicts how the outputs should change as the inputs change.[111] Both classifiers and regression learners can be viewed as "function approximators" trying to learn an unknown (possibly implicit)
        function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, "spam" or "not spam". Computational learning theory can assess learners by computational complexity,
        by
        sample complexity (how much data is required), or by other notions of optimization.[112] In reinforcement learning[113] the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and
        punishments
        to form a strategy for operating in its problem space.</p>
      <br>
    </section>

    <section>
      <h2>Natural language processing</h2>
      <p>Natural language processing[114] (NLP) gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of
        knowledge
        directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering[115] and machine translation.[116] Many current
        approaches use word co-occurrence frequencies to construct syntactic representations of text. "Keyword spotting" strategies for search are popular and scalable but dumb; a search query for "dog" might only match documents with the literal
        word
        "dog" and miss a document with the word "poodle". "Lexical affinity" strategies use the occurrence of words such as "accident" to assess the sentiment of a document. Modern statistical NLP approaches can combine all these strategies as well
        as
        others, and often achieve acceptable accuracy at the page or paragraph level, but continue to lack the semantic understanding required to classify isolated sentences well. Besides the usual difficulties with encoding semantic commonsense
        knowledge, existing semantic NLP sometimes scales too poorly to be viable in business applications. Beyond semantic NLP, the ultimate goal of "narrative" NLP is to embody a full understanding of commonsense reasoning.[117]</p>
      <br>
    </section>

    <section>
      <h2>Perception</h2>
      <p>Machine perception[118] is the ability to use input from sensors (such as cameras (visible spectrum or infrared), microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world.
        Applications
        include speech recognition,[119] facial recognition, and object recognition.[120] Computer vision is the ability to analyze visual input. Such input is usually ambiguous; a giant, fifty-meter-tall pedestrian far away may produce exactly the
        same pixels as a nearby normal-sized pedestrian, requiring the AI to judge the relative likelihood and reasonableness of different interpretations, for example by using its "object model" to assess that fifty-meter pedestrians do not
        exist.[121]</p>
      <br>
    </section>

    <section>
      <h2>Motion and manipulation</h2>
      <p>AI is heavily used in robotics.[122] Advanced robotic arms and other industrial robots, widely used in modern factories, can learn from experience how to move efficiently despite the presence of friction and gear slippage.[123] A modern
        mobile
        robot, when given a small, static, and visible environment, can easily determine its location and map its environment; however, dynamic environments, such as (in endoscopy) the interior of a patient's breathing body, pose a greater challenge.
        Motion planning is the process of breaking down a movement task into "primitives" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an
        object.[124][125][126] Moravec's paradox generalizes that low-level sensorimotor skills that humans take for granted are, counterintuitively, difficult to program into a robot; the paradox is named after Hans Moravec, who stated in 1988 that
        "it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility".[127][128]
        This is attributed to the fact that, unlike checkers, physical dexterity has been a direct target of natural selection for millions of years.[129]</p>
      <br>
    </section>

    <section>
      <h2>Social intelligence</h2>
      <p>Moravec's paradox can be extended to many forms of social intelligence.[131][132] Distributed multi-agent coordination of autonomous vehicles remains a difficult problem.[133] Affective computing is an interdisciplinary umbrella that
        comprises
        systems which recognize, interpret, process, or simulate human affects.[134][135][136] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal affect analysis (see multimodal
        sentiment
        analysis), wherein AI classifies the affects displayed by a videotaped subject.[137]

        In the long run, social skills and an understanding of human emotion and game theory would be valuable to a social agent. Being able to predict the actions of others by understanding their motives and emotional states would allow an agent to
        make better decisions. Some computer systems mimic human emotion and expressions to appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.[138] Similarly, some virtual
        assistants are programmed to speak conversationally or even to banter humorously; this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.[139]</p>
      <br>
    </section>

    <section>
      <h2>General intelligence</h2>
      <p>Historically, projects such as the Cyc knowledge base (1984–) and the massive Japanese Fifth Generation Computer Systems initiative (1982–1992) attempted to cover the breadth of human cognition. These early projects failed to escape the
        limitations of non-quantitative symbolic logic models and, in retrospect, greatly underestimated the difficulty of cross-domain AI. Nowadays, the vast majority of current AI researchers work instead on tractable "narrow AI" applications (such
        as medical diagnosis or automobile navigation).[140] Many researchers predict that such "narrow AI" work in different individual domains will eventually be incorporated into a machine with artificial general intelligence (AGI), combining most
        of the narrow skills mentioned in this article and at some point even exceeding human ability in most or all these areas.[19][141] Many advances have general, cross-domain significance. One high-profile example is that DeepMind in the 2010s
        developed a "generalized artificial intelligence" that could learn many diverse Atari games on its own, and later developed a variant of the system which succeeds at sequential learning.[142][143][144] Besides transfer learning,[145]
        hypothetical AGI breakthroughs could include the development of reflective architectures that can engage in decision-theoretic metareasoning, and figuring out how to "slurp up" a comprehensive knowledge base from the entire unstructured
        Web.[6]
        Some argue that some kind of (currently-undiscovered) conceptually straightforward, but mathematically difficult, "Master Algorithm" could lead to AGI.[146] Finally, a few "emergent" approaches look to simulating human intelligence extremely
        closely, and believe that anthropomorphic features like an artificial brain or simulated child development may someday reach a critical point where general intelligence emerges.[147][148]

        Many of the problems in this article may also require general intelligence, if machines are to solve the problems as well as people do. For example, even specific straightforward tasks, like machine translation, require that a machine read
        and
        write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered
        "AI-complete", because all of these problems need to be solved simultaneously in order to reach human-level machine performance.</p>
      <br>
    </section>
  </main>
</body>

</html>
